{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "lines = open(\"movie_lines.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split('\\n')\n",
    "conversations = open(\"movie_conversations.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
       " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
       " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
       " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
       " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
       " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map ecah line with it's id.\n",
    "id_line = {} # Our dictionary\n",
    "for line in lines:\n",
    "    line = line.split(' +++$+++ ')\n",
    "    if len(line) == 5:\n",
    "        id_line[line[0]] = line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the conversations\n",
    "conversations_ids = []\n",
    "for x in conversations[:-1]:\n",
    "    x = x.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conversations_ids.append(x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_ids[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data ready in input/output format\n",
    "ques = []\n",
    "ans = []\n",
    "for x in conversations_ids:\n",
    "    for i in range(len(x) -1):\n",
    "        ques.append(id_line[x[i]])\n",
    "        ans.append(id_line[x[i+1]])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " 'Not the hacking and gagging and spitting part.  Please.',\n",
       " \"You're asking me out.  That's so cute. What's your name again?\",\n",
       " \"No, no, it's my fault -- we didn't have a proper introduction ---\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " 'Not the hacking and gagging and spitting part.  Please.',\n",
       " \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
       " 'Forget it.',\n",
       " 'Cameron.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_clean = [clean_text(q) for q in ques]\n",
    "answers_clean = [clean_text(q) for q in ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
       " 'well i thought we would start with pronunciation if that is okay with you',\n",
       " 'not the hacking and gagging and spitting part  please',\n",
       " 'you are asking me out  that is so cute what is your name again',\n",
       " \"no no it's my fault  we didn't have a proper introduction \"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_clean[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionar to map each word to it's feauency.\n",
    "word_dict = {}\n",
    "for q in questions_clean:\n",
    "    for word in q.split(\" \"):\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1\n",
    "\n",
    "for q in answers_clean:\n",
    "    for word in q.split(\" \"):\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 180591,\n",
       " 'goofiest': 1,\n",
       " 'dears': 3,\n",
       " 'starvation': 6,\n",
       " \"jew's\": 7,\n",
       " 'drugs!': 7,\n",
       " 'itouch!': 2,\n",
       " 'savannah': 4,\n",
       " 'guggenheim': 1,\n",
       " 'polymerization': 4,\n",
       " 'chango': 2,\n",
       " \"absofuckin'\": 1,\n",
       " 'flashback': 5,\n",
       " 'dymshitz': 4,\n",
       " 'admit': 316,\n",
       " 'casemistmr': 2,\n",
       " 'accommodating': 2,\n",
       " \"'foolproof'\": 1,\n",
       " 'niner': 8,\n",
       " 'engineering': 50,\n",
       " \"susan's\": 9,\n",
       " 'diabolical': 3,\n",
       " 'reclose': 7,\n",
       " 'investigate': 40,\n",
       " 'kraal': 2,\n",
       " 'hochmut': 4,\n",
       " 'romantic': 140,\n",
       " 'thewall': 2,\n",
       " 'uokayu': 6,\n",
       " 'drscott': 2,\n",
       " 'fangs': 16,\n",
       " 'distention': 2,\n",
       " \"twombley's\": 8,\n",
       " 'waaaaahhhhhhhhhh!': 1,\n",
       " \"crazier'n\": 3,\n",
       " 'ticket!': 7,\n",
       " 'wormsticker': 1,\n",
       " 'glanced': 2,\n",
       " 'lapse': 10,\n",
       " 'animation': 5,\n",
       " 'veto': 7,\n",
       " 'failed!': 1,\n",
       " 'transcribing': 1,\n",
       " 'ishow': 1,\n",
       " 'maroon': 25,\n",
       " 'waist': 10,\n",
       " 'buttered': 7,\n",
       " \"thx's\": 2,\n",
       " 'mysteri': 1,\n",
       " 'paree': 2,\n",
       " 'the\\t\\t\\t\\t*': 1,\n",
       " 'concussions': 1,\n",
       " 'cramming': 3,\n",
       " 'einstein': 31,\n",
       " 'linnea': 7,\n",
       " 'gamble': 55,\n",
       " \"dolores's\": 1,\n",
       " 'convinced': 93,\n",
       " \"gaulle's\": 2,\n",
       " 'consar': 1,\n",
       " 'prepaid': 1,\n",
       " 'brian!': 4,\n",
       " 'nightmute!': 1,\n",
       " 'sheshe': 9,\n",
       " 'smythe!': 5,\n",
       " 'marina': 6,\n",
       " 'distillers': 2,\n",
       " 'confidenceu': 2,\n",
       " 'directing': 20,\n",
       " 'prototype!': 1,\n",
       " 'purely': 39,\n",
       " 'hustle': 37,\n",
       " 'quid!': 1,\n",
       " 'know\\t\\t\\t\\t': 2,\n",
       " \"steven's\": 12,\n",
       " 'manifest': 18,\n",
       " 'tolchocked': 2,\n",
       " 'trad0': 2,\n",
       " 'insectcreature': 1,\n",
       " 'who\\t\\t\\t\\t\\t\\t\\t*': 2,\n",
       " 'morphology': 1,\n",
       " 'round!': 4,\n",
       " \"l'unica\": 1,\n",
       " 'fatass': 3,\n",
       " '105353': 2,\n",
       " 'sorryso': 2,\n",
       " 'glycotetraglycine': 1,\n",
       " 'steal': 275,\n",
       " 'thusly': 1,\n",
       " 'untainted': 1,\n",
       " \"interview's\": 2,\n",
       " 'thoughtful': 25,\n",
       " 'resentment': 6,\n",
       " 'uheyu!': 1,\n",
       " 'barring': 5,\n",
       " 'liberator': 6,\n",
       " 'ninth': 32,\n",
       " 'halfhorse': 1,\n",
       " 'mookie': 35,\n",
       " 'spartacus': 2,\n",
       " '[because': 1,\n",
       " \"'homecoming'\": 2,\n",
       " \"baiit's\": 1,\n",
       " 'odette': 7,\n",
       " 'uncommunicative': 2,\n",
       " \"devices'!\": 1,\n",
       " 'twentyfive': 176,\n",
       " 'craziest': 12,\n",
       " 'nasal': 26,\n",
       " 'daylights': 1,\n",
       " 'mantle': 2,\n",
       " \"l'orange\": 1,\n",
       " 'surprised': 300,\n",
       " 'abyss': 3,\n",
       " 'backor': 2,\n",
       " 'traveling': 59,\n",
       " 'ariane': 1,\n",
       " 'nastiness': 8,\n",
       " 'valdez': 2,\n",
       " 'honoring': 2,\n",
       " 'fanning': 1,\n",
       " '[not': 5,\n",
       " 'cl': 2,\n",
       " \"'it\": 9,\n",
       " 'rangers': 37,\n",
       " 'draws': 17,\n",
       " 'ernhart': 2,\n",
       " 'feedler': 2,\n",
       " 'pokrif': 2,\n",
       " 'sprouted': 2,\n",
       " 'mph': 26,\n",
       " 'jesusmi': 2,\n",
       " 'dishwasher': 7,\n",
       " 'bbelieve': 1,\n",
       " 'drenched': 6,\n",
       " 'egomaniacal': 4,\n",
       " 'iii': 78,\n",
       " 'flashlight': 17,\n",
       " 'race!!': 1,\n",
       " 'ishisname': 15,\n",
       " \"so'\": 3,\n",
       " 'regroup': 3,\n",
       " \"lyndsey's\": 2,\n",
       " \"shit'\": 1,\n",
       " 'kickthecan': 1,\n",
       " 'upon': 196,\n",
       " 'loudest': 1,\n",
       " 'blowjobs': 4,\n",
       " 'johnnyo': 4,\n",
       " 'controls!': 1,\n",
       " 'kopalskis': 2,\n",
       " 'prizes!': 1,\n",
       " 'wrestling!': 1,\n",
       " 'oneta': 1,\n",
       " 'peopleyou': 1,\n",
       " 'pacing': 1,\n",
       " 'overreact': 5,\n",
       " 'fastforward': 1,\n",
       " 'errands': 14,\n",
       " 'approach': 89,\n",
       " \"'sixsixsix'\": 3,\n",
       " \"buildin'\": 6,\n",
       " \"morgana's\": 2,\n",
       " 'humphrey': 6,\n",
       " 'wellyes': 1,\n",
       " 'warlord': 2,\n",
       " 'vein!': 1,\n",
       " 'plugget': 2,\n",
       " 'forever!': 12,\n",
       " \"startin'\": 27,\n",
       " 'finches': 5,\n",
       " 'hosts': 5,\n",
       " \"'look\": 3,\n",
       " 'overseeing': 2,\n",
       " 'wallis': 1,\n",
       " 'granview': 1,\n",
       " 'ehhh': 2,\n",
       " 'pure': 141,\n",
       " 'pissant': 11,\n",
       " 'azteclike': 1,\n",
       " 'scaled': 1,\n",
       " 'taanab': 1,\n",
       " 'dane': 5,\n",
       " 'lenses': 7,\n",
       " 'courtesycompeers': 1,\n",
       " 'marcie': 17,\n",
       " 'brazenitthrough': 2,\n",
       " \"reputation's\": 2,\n",
       " 'vale': 5,\n",
       " 'minitus': 1,\n",
       " 'sorely': 2,\n",
       " 'wold': 1,\n",
       " 'berchtesgarten': 1,\n",
       " 'teletype': 5,\n",
       " \"learn't\": 1,\n",
       " 'wordy': 2,\n",
       " '50000': 6,\n",
       " 'months': 889,\n",
       " 'dibble': 1,\n",
       " 'earmuffs': 1,\n",
       " 'accelerators': 2,\n",
       " 'legislation': 6,\n",
       " 'poldek': 1,\n",
       " '&quotoh': 1,\n",
       " 'velcome!': 1,\n",
       " 'depressurize!': 1,\n",
       " 'occurrence': 8,\n",
       " 'flamer': 1,\n",
       " 'wooten': 1,\n",
       " 'stuffy': 20,\n",
       " 'baywatch': 2,\n",
       " \"psychiatrist's\": 2,\n",
       " 'verauh': 1,\n",
       " 'consultant': 16,\n",
       " 'testimony': 86,\n",
       " 'nnnnnnn!': 1,\n",
       " 'harry!': 27,\n",
       " 'samethat': 1,\n",
       " 'fuckrag': 2,\n",
       " 'forger': 5,\n",
       " 'kilt': 6,\n",
       " 'moldavians': 2,\n",
       " 'backups': 3,\n",
       " \"fuhrer's\": 3,\n",
       " 'cynics': 6,\n",
       " 'lessen': 5,\n",
       " 'marchena': 2,\n",
       " 'judgment!': 4,\n",
       " 'americau': 1,\n",
       " 'trucoat!': 2,\n",
       " 'chinwag': 2,\n",
       " 'mmmy': 3,\n",
       " 'yeahjust': 1,\n",
       " 'carnes': 5,\n",
       " \"correspond'\": 2,\n",
       " 'alhe': 2,\n",
       " 'delmarthough': 1,\n",
       " 'facetious': 2,\n",
       " 'pokus': 1,\n",
       " 'cafeteria': 18,\n",
       " '5099027': 2,\n",
       " 'expeck': 2,\n",
       " 'candy!': 5,\n",
       " 'soul!': 7,\n",
       " 'hairgorgeous': 2,\n",
       " 'heathcliffe': 4,\n",
       " 'aheadpunch': 1,\n",
       " \"clear'\": 2,\n",
       " 'blodsum': 2,\n",
       " 'scorn': 3,\n",
       " 'otic': 3,\n",
       " 'twentyeightytwo': 1,\n",
       " 'goddamndest': 1,\n",
       " 'lawyers': 116,\n",
       " 'crackerbox': 2,\n",
       " \"wouldrugged'\": 2,\n",
       " 'distractedly': 2,\n",
       " 'colcheck': 1,\n",
       " 'entitled': 69,\n",
       " 'vr': 6,\n",
       " 'wasting': 132,\n",
       " 'whoohoo!': 3,\n",
       " \"'nigger\": 2,\n",
       " 'melissa!': 1,\n",
       " \"'wings\": 2,\n",
       " 'ounce': 23,\n",
       " \"picker's\": 1,\n",
       " \"nazisandthelesbosthatlove'em\": 1,\n",
       " 'exkgb': 2,\n",
       " 'wallpaper': 21,\n",
       " 'reshoot': 3,\n",
       " 'utellingu': 1,\n",
       " 'giulia': 10,\n",
       " 'straggling': 1,\n",
       " 'observatory': 9,\n",
       " 'steakhead': 2,\n",
       " 'tongue!': 4,\n",
       " 'agitate': 1,\n",
       " \"dyin's\": 3,\n",
       " 'shortsightedness': 1,\n",
       " 'moisturize': 3,\n",
       " 'loosen': 24,\n",
       " 'gushing': 3,\n",
       " 'intricacies': 2,\n",
       " 'oha': 4,\n",
       " 'retook': 2,\n",
       " \"carlos'\": 1,\n",
       " \"programme's\": 1,\n",
       " 'salinas': 2,\n",
       " 'bur': 8,\n",
       " 'dh1': 1,\n",
       " 'chump': 26,\n",
       " 'fizzled': 3,\n",
       " 'haircut': 41,\n",
       " 'saidthe': 3,\n",
       " 'breeders': 4,\n",
       " 'girlyfriend': 1,\n",
       " '$3350': 2,\n",
       " 'hoot!': 1,\n",
       " 'sexogerarian': 2,\n",
       " 'dishonourable': 2,\n",
       " 'methat': 7,\n",
       " 'southwest': 15,\n",
       " \"clawsen's\": 2,\n",
       " 'amyi': 1,\n",
       " 'butterfingers!': 1,\n",
       " 'quivers': 1,\n",
       " 'ovah': 2,\n",
       " 'comix': 3,\n",
       " 'increases': 3,\n",
       " 'shifting': 8,\n",
       " 'reevaluated': 2,\n",
       " 'junction': 20,\n",
       " 'chuckled': 1,\n",
       " 'contacting': 9,\n",
       " 'ballard': 9,\n",
       " 'peestains': 1,\n",
       " 'overriding': 1,\n",
       " 'dana!': 3,\n",
       " 'dangerous!': 6,\n",
       " \"consultation'\": 2,\n",
       " 'extroverts': 2,\n",
       " 'ashamed!': 3,\n",
       " 'calculate': 14,\n",
       " 'springwood': 7,\n",
       " '$700': 2,\n",
       " \"traffic's\": 4,\n",
       " 'bouchon': 4,\n",
       " 'bog': 6,\n",
       " 'jehovah': 1,\n",
       " \"kaplans'\": 1,\n",
       " 'cocktail!': 2,\n",
       " 'spirits': 67,\n",
       " 'assyou': 2,\n",
       " 'ilovei': 1,\n",
       " \"wellcouldn't\": 1,\n",
       " 'alvarez': 6,\n",
       " 'grusinskaya': 19,\n",
       " \"wouldarling'\": 1,\n",
       " 'gladiator!': 1,\n",
       " 'summation': 5,\n",
       " 'performance!': 7,\n",
       " 'verification': 6,\n",
       " 'sky!': 3,\n",
       " \"frit's\": 1,\n",
       " 'nicola': 7,\n",
       " \"n'you\": 3,\n",
       " 'whattalk': 1,\n",
       " 'aaaaeeehhhhg!!!': 1,\n",
       " \"whore's\": 4,\n",
       " 'stonewall': 4,\n",
       " 'signifies': 3,\n",
       " 'moulting': 4,\n",
       " 'uconflictu': 2,\n",
       " 'migratory': 4,\n",
       " 'ruths': 1,\n",
       " 'somethin': 12,\n",
       " \"homicide's\": 3,\n",
       " 'rayisha': 3,\n",
       " 'truancy': 2,\n",
       " 'kiloton': 1,\n",
       " 'assasin': 2,\n",
       " 'rache!': 1,\n",
       " 'uthen': 3,\n",
       " \"'in'\": 2,\n",
       " 'billfoldi': 1,\n",
       " 'situation!': 2,\n",
       " 'holden': 10,\n",
       " 'please!!!': 2,\n",
       " 'genesee': 1,\n",
       " 'crusades': 10,\n",
       " 'pilletti': 9,\n",
       " 'twoperson': 2,\n",
       " 'reiterate': 4,\n",
       " 'okayyou': 3,\n",
       " 'hundredu': 2,\n",
       " 'beeni': 3,\n",
       " 'isis': 8,\n",
       " 'vedder': 2,\n",
       " 'plaintiffs': 11,\n",
       " 'honestly': 126,\n",
       " 'cooled': 6,\n",
       " 'cones': 1,\n",
       " 'winner!': 4,\n",
       " 'stimulated': 3,\n",
       " 'dreamt': 20,\n",
       " 'thathow': 2,\n",
       " 'nuke': 27,\n",
       " 'holder': 12,\n",
       " \"bore's\": 2,\n",
       " 'psychoses': 2,\n",
       " \"oan't\": 1,\n",
       " 'trunks': 10,\n",
       " 'abbott': 21,\n",
       " 'breezes': 3,\n",
       " \"ahthere's\": 1,\n",
       " 'family&quot': 2,\n",
       " 'althea': 5,\n",
       " 'haitianpuerto': 1,\n",
       " 'lawnmowers': 1,\n",
       " 'auught!': 2,\n",
       " 'successes': 4,\n",
       " 'tables': 51,\n",
       " 'uhit': 2,\n",
       " 'leeno': 2,\n",
       " 'extremus': 2,\n",
       " 'iuhregressed': 1,\n",
       " 'ulcer': 13,\n",
       " 'starshaped': 2,\n",
       " 'saymcgann': 2,\n",
       " 'freylag': 2,\n",
       " 'cigar!': 5,\n",
       " 'bsomething': 3,\n",
       " 'sprawl': 3,\n",
       " 'immodest': 3,\n",
       " 'visor': 3,\n",
       " \"thief's\": 1,\n",
       " 'contestyou': 1,\n",
       " 'headon': 2,\n",
       " 'schumann': 18,\n",
       " \"'night\": 13,\n",
       " 'showerbefore': 1,\n",
       " 'playskool': 1,\n",
       " \"eagle's\": 2,\n",
       " 'comfortably': 7,\n",
       " 'shotu': 1,\n",
       " 'vince!!!': 1,\n",
       " 'swole': 3,\n",
       " 'strikes': 51,\n",
       " 'procedures': 36,\n",
       " 'aide': 12,\n",
       " 'sucking': 49,\n",
       " 'fluffand': 1,\n",
       " 'golden': 83,\n",
       " 'upstage': 2,\n",
       " \"castle's\": 4,\n",
       " 'want*and': 1,\n",
       " 'stegosaur': 2,\n",
       " 'differences': 24,\n",
       " 'ghost!': 2,\n",
       " 'tosserturner': 1,\n",
       " 'outrage!': 3,\n",
       " 'percocets': 1,\n",
       " 'goofy': 38,\n",
       " 'himlandon': 1,\n",
       " 'sonofabitches': 2,\n",
       " 'pare': 4,\n",
       " 'timefucking': 1,\n",
       " 'uloseru': 1,\n",
       " 'imagination!': 3,\n",
       " \"'cost\": 1,\n",
       " 'precaution': 15,\n",
       " 'sew': 16,\n",
       " 'beautywith': 1,\n",
       " 'thighs': 16,\n",
       " 'mmmmore': 2,\n",
       " 'fairy': 59,\n",
       " 'gy': 1,\n",
       " 'bluff': 50,\n",
       " 'knowhang': 2,\n",
       " \"amber's\": 1,\n",
       " 'yazoo!': 2,\n",
       " 'broomstick': 8,\n",
       " 'simian': 5,\n",
       " 'inflicted': 2,\n",
       " 'sizzler': 3,\n",
       " '[if': 1,\n",
       " 'maniac': 25,\n",
       " 'pinesol': 1,\n",
       " 'canvass': 5,\n",
       " 'brushoff': 3,\n",
       " 'thorwalds': 3,\n",
       " 'dafur': 2,\n",
       " 'plumbing': 26,\n",
       " 'businessman': 59,\n",
       " 'adultery': 4,\n",
       " 'noooooo!': 1,\n",
       " 'pain': 390,\n",
       " 'quadruple': 1,\n",
       " 'cheerleader': 7,\n",
       " 'meantempered': 1,\n",
       " 'androgenous': 1,\n",
       " 'chanting': 6,\n",
       " 'gothic': 6,\n",
       " 'sisler': 5,\n",
       " 'compensate!': 2,\n",
       " 'sayhoney': 1,\n",
       " 'chestcutter': 6,\n",
       " 'kelson': 12,\n",
       " 'grayest': 2,\n",
       " 'tonightwe': 1,\n",
       " 'skylar': 4,\n",
       " 'merciful!': 1,\n",
       " 'presently!': 2,\n",
       " 'isthmus': 4,\n",
       " 'kiddy': 1,\n",
       " 'uneedsu': 2,\n",
       " 'uapologizeu': 1,\n",
       " 'mirrored': 1,\n",
       " 'limping': 14,\n",
       " 'shitbag': 2,\n",
       " 'canaveral': 4,\n",
       " 'hampen': 1,\n",
       " 'sakes': 25,\n",
       " 'blowed': 2,\n",
       " 'cartographer': 1,\n",
       " 'corrick': 1,\n",
       " 'manufactured': 6,\n",
       " 'hellhole': 10,\n",
       " 'siders': 2,\n",
       " 'fda': 6,\n",
       " 'lovesick': 2,\n",
       " 'sands': 3,\n",
       " 'heriditary': 1,\n",
       " 'prefered': 2,\n",
       " 'lsd': 10,\n",
       " 'pickaninny': 2,\n",
       " 'quel': 2,\n",
       " \"rosebud's\": 2,\n",
       " '755': 3,\n",
       " 'tira': 4,\n",
       " 'smile': 157,\n",
       " 'gets!': 2,\n",
       " 'physician!': 1,\n",
       " 'ufilmu': 2,\n",
       " 'pastry': 11,\n",
       " 'rudolfo': 4,\n",
       " 'appendix': 6,\n",
       " 'butte': 4,\n",
       " 'ballast': 14,\n",
       " 'gaddafi': 4,\n",
       " 'blanc': 8,\n",
       " 'reanimation': 2,\n",
       " 'shame': 154,\n",
       " 'lecturing': 9,\n",
       " 'zero!!': 2,\n",
       " \"'honored\": 2,\n",
       " 'yessure': 1,\n",
       " 'butif': 1,\n",
       " 'truth\\t\\t\\t': 1,\n",
       " 'berkowitz': 6,\n",
       " 'syrup': 8,\n",
       " '274f': 1,\n",
       " 'masterfully': 2,\n",
       " \"o'conner\": 1,\n",
       " 'wingspan': 1,\n",
       " \"lewis's\": 1,\n",
       " 'carea': 1,\n",
       " \"don'tplease\": 1,\n",
       " 'hitchers': 1,\n",
       " 'stars': 182,\n",
       " 'whata': 4,\n",
       " \"university's\": 2,\n",
       " 'indiscreet': 7,\n",
       " 'trenton!': 2,\n",
       " \"peoples'\": 3,\n",
       " 'entourage': 7,\n",
       " 'cossack': 2,\n",
       " \"bleed'm\": 1,\n",
       " 'moments!': 1,\n",
       " 'soundground': 1,\n",
       " 'uthatu': 58,\n",
       " 'pbr': 3,\n",
       " 'placebut': 5,\n",
       " 'eared': 2,\n",
       " 'christiandom': 3,\n",
       " 'decamped': 2,\n",
       " 'alarming': 6,\n",
       " 'trigger': 86,\n",
       " 'awol': 5,\n",
       " 'alcoholics': 1,\n",
       " 'hopelessyou': 1,\n",
       " 'underwater!': 3,\n",
       " 'aux': 1,\n",
       " 'flavor': 13,\n",
       " 'thoughjust': 1,\n",
       " 'jessmon': 1,\n",
       " '483': 2,\n",
       " 'babysitter': 20,\n",
       " 'hbo': 5,\n",
       " 'odometers': 2,\n",
       " 'itwe': 9,\n",
       " 'possess': 25,\n",
       " 'diagnosis!': 1,\n",
       " 'sanford': 3,\n",
       " 'simonovitch': 3,\n",
       " 'siphoning': 1,\n",
       " 'bombouts': 1,\n",
       " 'cather': 1,\n",
       " 'reacted': 6,\n",
       " \"gabe's\": 2,\n",
       " 'burning': 136,\n",
       " 'montague': 11,\n",
       " 'rabbinic': 2,\n",
       " 'spaceships': 1,\n",
       " \"butt's\": 3,\n",
       " 'sami': 12,\n",
       " 'boyfriend!': 2,\n",
       " 'curacao': 4,\n",
       " 'dishonesty': 3,\n",
       " \"drinking's\": 2,\n",
       " 'unstuck': 1,\n",
       " 'supremacists': 1,\n",
       " 'tankers': 4,\n",
       " 'downstairs!': 3,\n",
       " 'catfood': 2,\n",
       " 'money]': 2,\n",
       " \"counter's\": 3,\n",
       " 'stock': 144,\n",
       " 'gals!': 2,\n",
       " 'unclean': 3,\n",
       " 'iate': 1,\n",
       " 'rebuild': 7,\n",
       " 'hatchling': 1,\n",
       " 'die!!': 2,\n",
       " 'ankara': 3,\n",
       " 'levene': 4,\n",
       " 'luckiest': 10,\n",
       " 'phoenix': 37,\n",
       " 'childs!!': 1,\n",
       " 'bike': 46,\n",
       " 'approximation': 2,\n",
       " 'beggars': 18,\n",
       " 'cooperate': 71,\n",
       " 'ugou': 5,\n",
       " 'surface': 88,\n",
       " 'levitate': 4,\n",
       " 'riffraff': 2,\n",
       " 'dano': 3,\n",
       " 'irs': 31,\n",
       " 'ubut': 1,\n",
       " 'canonize': 2,\n",
       " 'break!': 23,\n",
       " 'wives': 67,\n",
       " 'whatzernamethe': 1,\n",
       " 'dads': 7,\n",
       " 'leonitchka!': 3,\n",
       " \"ind'n\": 1,\n",
       " 'ubeeeutel': 2,\n",
       " 'stupe': 3,\n",
       " \"game's\": 27,\n",
       " 'undamaged': 2,\n",
       " 'saythe': 2,\n",
       " 'protect': 329,\n",
       " \"lebanon'\": 2,\n",
       " '*migrant*': 1,\n",
       " 'splattered': 8,\n",
       " 'naps': 6,\n",
       " 'handpainted': 1,\n",
       " 'notcomplaining': 2,\n",
       " 'skirt': 39,\n",
       " 'erect': 6,\n",
       " 'soundangerous': 1,\n",
       " 'oxygenstarved': 2,\n",
       " 'qonna': 2,\n",
       " \"yeahwouldn't\": 2,\n",
       " 'shucks': 13,\n",
       " 'ragged!': 1,\n",
       " 'mackelway': 28,\n",
       " 'equanimity': 1,\n",
       " 'warp': 62,\n",
       " 'guess': 2833,\n",
       " 'pretending': 77,\n",
       " 'asked': 1209,\n",
       " \"men'sroom\": 1,\n",
       " 'outdoor': 3,\n",
       " 'uhelpu!': 2,\n",
       " 'offensive!': 1,\n",
       " 'cuteand': 2,\n",
       " 'gerta': 3,\n",
       " 'groin': 3,\n",
       " 'heres': 3,\n",
       " 'registrar': 1,\n",
       " 'intense!': 1,\n",
       " \"tossin'\": 4,\n",
       " 'saidthis': 1,\n",
       " 'lazlo': 3,\n",
       " 'polymerized': 6,\n",
       " 'you!!': 34,\n",
       " 'hanks': 1,\n",
       " 'scullery': 1,\n",
       " 'lipped': 1,\n",
       " 'originates': 1,\n",
       " 'groundhog': 1,\n",
       " 'tom!': 16,\n",
       " 'joeat': 1,\n",
       " 'romari\\tif': 1,\n",
       " 'thoughtoh': 2,\n",
       " \"rodent's\": 1,\n",
       " 'juliemy': 1,\n",
       " 'wellseeming': 2,\n",
       " 'couldda': 1,\n",
       " 'attemptted!': 1,\n",
       " 'accelerate': 6,\n",
       " 'fortuny': 2,\n",
       " 'holy': 303,\n",
       " 'slippers!': 2,\n",
       " 'ansila': 1,\n",
       " 'classicstatus': 1,\n",
       " 'rainman': 2,\n",
       " 'editor': 67,\n",
       " \"depen'\": 1,\n",
       " 'simoon': 1,\n",
       " 'mythologies': 2,\n",
       " 'unfair': 57,\n",
       " 'homeopathy': 1,\n",
       " 'slamming': 7,\n",
       " \"yellin'\": 11,\n",
       " 'j&b': 1,\n",
       " 'perambulation': 2,\n",
       " 'happens': 768,\n",
       " 'isare': 1,\n",
       " 'fair!!!': 2,\n",
       " 'reprieve': 12,\n",
       " 'crossmounted': 1,\n",
       " 'puddleville': 1,\n",
       " 'abouti': 3,\n",
       " 'commies': 11,\n",
       " 'september': 34,\n",
       " \"denham's\": 1,\n",
       " 'dulaney': 8,\n",
       " 'venza': 7,\n",
       " 'mullethead': 1,\n",
       " 'provider': 1,\n",
       " 'stifler!!': 1,\n",
       " 'spicks': 2,\n",
       " 'brenners': 6,\n",
       " 'easy!!': 1,\n",
       " 'iund': 1,\n",
       " \"wallace's\": 5,\n",
       " 'jos': 23,\n",
       " 'digestu': 1,\n",
       " 'psycho': 88,\n",
       " 'sadists': 4,\n",
       " 'cates': 10,\n",
       " 'erratic': 9,\n",
       " 'specks': 2,\n",
       " 'amb': 1,\n",
       " 'newberry': 4,\n",
       " 'entertainers': 6,\n",
       " 'cccannot': 2,\n",
       " 'crossed': 71,\n",
       " 'uhsurewhen': 2,\n",
       " 'twerp': 5,\n",
       " 'sorgfaltic': 2,\n",
       " 'fevers': 4,\n",
       " 'radical': 26,\n",
       " 'multigenerational': 1,\n",
       " 'blazemakes': 1,\n",
       " 'churchyou': 2,\n",
       " 'steubing': 1,\n",
       " 'getter': 2,\n",
       " 'slovenly': 1,\n",
       " 'apprentices': 3,\n",
       " 'hackensack': 1,\n",
       " \"'thing'\": 1,\n",
       " 'buds': 1,\n",
       " 'ujudgeu': 2,\n",
       " 'honored': 49,\n",
       " \"chloe's\": 2,\n",
       " \"'insignificance'\": 1,\n",
       " 'refitted': 1,\n",
       " 'tate': 9,\n",
       " 'noncom': 2,\n",
       " 'meanthings': 2,\n",
       " 'navibeacon': 1,\n",
       " 'twat': 1,\n",
       " 'oxygen!': 2,\n",
       " 'screech': 3,\n",
       " 'ibuzz': 1,\n",
       " 'impolite': 10,\n",
       " 'intern': 6,\n",
       " 'skywalker': 18,\n",
       " \"crispers'\": 1,\n",
       " 'nostalgia': 9,\n",
       " 'flaky': 1,\n",
       " 'repeat!': 5,\n",
       " 'gregory]': 1,\n",
       " 'stickaneedlein': 2,\n",
       " 'screaming': 132,\n",
       " 'meetshe': 1,\n",
       " \"truck's\": 4,\n",
       " 'occured': 6,\n",
       " 'hello': 1310,\n",
       " 'kitchenette': 2,\n",
       " 'rouge': 7,\n",
       " 'umbilicus': 1,\n",
       " 'n': 34,\n",
       " 'pues!': 1,\n",
       " 'tiredness': 1,\n",
       " 'hippopotamic': 1,\n",
       " 'sight!': 7,\n",
       " 'roomu': 2,\n",
       " 'dunaway': 5,\n",
       " 'deflating': 1,\n",
       " 'ape!': 4,\n",
       " 'grandchildren': 18,\n",
       " \"hun'\": 2,\n",
       " 'madmen': 9,\n",
       " 'immobilized': 1,\n",
       " 'shareholders': 2,\n",
       " 'shitbreak': 1,\n",
       " 'waterfalls': 1,\n",
       " 'hydrated': 1,\n",
       " 'igiven': 2,\n",
       " 'wright': 8,\n",
       " \"jonah's\": 1,\n",
       " 'mistermed': 2,\n",
       " 'yearnings': 1,\n",
       " 'montell': 1,\n",
       " 'clitoris!': 4,\n",
       " 'eightone': 1,\n",
       " 'locust': 1,\n",
       " 'luck': 554,\n",
       " 'perhaps': 777,\n",
       " 'offworlders': 1,\n",
       " 'snoop!': 2,\n",
       " 'shredded': 5,\n",
       " 'timers': 8,\n",
       " 'gained': 26,\n",
       " 'tilt': 6,\n",
       " 'sink': 59,\n",
       " \"o'sheas\": 2,\n",
       " 'perfected': 3,\n",
       " 'violation': 53,\n",
       " 'air!!!': 1,\n",
       " 'committees': 8,\n",
       " 'doctore': 2,\n",
       " 'exwifewhen': 1,\n",
       " 'baking': 8,\n",
       " 'unhaunted': 2,\n",
       " 'waller': 1,\n",
       " 'burbage': 14,\n",
       " 'guzman': 2,\n",
       " 'cess': 1,\n",
       " 'fighten!': 1,\n",
       " 'lakei': 2,\n",
       " 'boardwalk': 14,\n",
       " 'raincoats': 3,\n",
       " 'shaman': 13,\n",
       " 'gotcha!': 4,\n",
       " 'crate!': 1,\n",
       " 'liverwurst': 3,\n",
       " 'bogged': 2,\n",
       " 'zayas': 2,\n",
       " \"fo'a\": 1,\n",
       " \"christianity's\": 2,\n",
       " 'ohime!': 2,\n",
       " 'low': 278,\n",
       " 'nguyen': 5,\n",
       " 'annette!': 5,\n",
       " 'occur': 48,\n",
       " 'unburned': 1,\n",
       " 'int': 80,\n",
       " \"'mattie\": 1,\n",
       " 'iceberg': 5,\n",
       " 'annikin': 1,\n",
       " 'preignition': 1,\n",
       " 'terminator!': 2,\n",
       " 'sixtynine': 7,\n",
       " 'abundance': 3,\n",
       " 'retrieved': 4,\n",
       " 'screwups': 4,\n",
       " 'strapping': 3,\n",
       " \"warin'\": 3,\n",
       " '&quotuh': 2,\n",
       " 'youbetcha': 2,\n",
       " '*yield*': 2,\n",
       " 'lift': 135,\n",
       " 'disbarred': 21,\n",
       " '237': 9,\n",
       " 'whacks': 2,\n",
       " 'knowhe': 2,\n",
       " 'gregoire': 11,\n",
       " \"'musta\": 2,\n",
       " 'bagful': 4,\n",
       " 'whatisit': 1,\n",
       " 'rollers!': 2,\n",
       " 'ez': 2,\n",
       " 'hwang': 2,\n",
       " 'misdirected': 2,\n",
       " 'sideration': 1,\n",
       " '!!!!!': 1,\n",
       " 'plopping': 1,\n",
       " 'uz': 1,\n",
       " 'sewickly': 2,\n",
       " 'roomunder': 2,\n",
       " 'smackdab': 2,\n",
       " 'brumby': 13,\n",
       " 'revolutionary': 26,\n",
       " 'sssssh': 2,\n",
       " 'choice!!': 2,\n",
       " 'addison!': 1,\n",
       " 'wayu!': 1,\n",
       " \"learnin'\": 7,\n",
       " 'inhurry': 1,\n",
       " 'gun!!': 1,\n",
       " 'floyd': 18,\n",
       " 'dressing': 60,\n",
       " 'blower!': 1,\n",
       " 'hi': 1176,\n",
       " 'sonofagun!': 1,\n",
       " 'haaaaathat': 1,\n",
       " 'although': 126,\n",
       " 'uunearthu': 2,\n",
       " 'herself!': 10,\n",
       " 'cheatingofficially': 2,\n",
       " 'boxy': 2,\n",
       " 'ears': 182,\n",
       " 'minutes!': 34,\n",
       " 'signazoid': 1,\n",
       " 'converges': 1,\n",
       " 'raid': 32,\n",
       " \"again'\": 2,\n",
       " 'flyinguallu': 1,\n",
       " 'sacrilege!': 1,\n",
       " \"doesn'tokay\": 1,\n",
       " 'yearold': 31,\n",
       " 'croce': 5,\n",
       " 'brown': 207,\n",
       " 'omnipotently': 2,\n",
       " 'ray!': 15,\n",
       " '4800': 3,\n",
       " 'designate': 3,\n",
       " 'run!': 36,\n",
       " 'prancing': 2,\n",
       " 'digit': 6,\n",
       " 'rinky': 1,\n",
       " 'tend': 62,\n",
       " '0504': 2,\n",
       " 'classticket': 2,\n",
       " 'temple': 43,\n",
       " '1948': 6,\n",
       " 'trashy': 3,\n",
       " 'yearher': 1,\n",
       " 'mutton': 2,\n",
       " 'donaldson': 2,\n",
       " 'libraries': 6,\n",
       " '$14380': 1,\n",
       " 'nostrovia': 2,\n",
       " 'mercier': 6,\n",
       " 'storing': 6,\n",
       " \"nigga's\": 1,\n",
       " 'fascinating': 69,\n",
       " 'tatum': 18,\n",
       " 'agreedi': 1,\n",
       " '_car_!': 1,\n",
       " 'andtruck': 1,\n",
       " 'dowell': 1,\n",
       " 'amlook': 1,\n",
       " 'unnatural': 18,\n",
       " \"diebold's\": 1,\n",
       " 'rose': 431,\n",
       " 'minor': 80,\n",
       " 'nevada': 25,\n",
       " 'designers': 3,\n",
       " 'admirable': 10,\n",
       " 'catwomanwhere': 1,\n",
       " 'bioqenic': 1,\n",
       " 'crowning': 2,\n",
       " 'himbecause': 3,\n",
       " 'collectorswe': 1,\n",
       " \"stranger's\": 3,\n",
       " 'antonucci': 6,\n",
       " 'atone': 3,\n",
       " 'morlocks!': 1,\n",
       " 'written!': 1,\n",
       " 'piddle': 2,\n",
       " 'schizoid': 4,\n",
       " 'jetting': 2,\n",
       " 'ingolstadt': 4,\n",
       " 'folksy': 2,\n",
       " 'browning': 5,\n",
       " 'armageddon': 5,\n",
       " 'crippledup': 1,\n",
       " 'lewis': 50,\n",
       " 'decorating': 5,\n",
       " '1888!': 1,\n",
       " 'jeet': 6,\n",
       " 'sucha': 1,\n",
       " \"'stab'\": 1,\n",
       " 'publicize': 5,\n",
       " 'fortyfourth': 2,\n",
       " 'simpleton': 5,\n",
       " 'rack': 25,\n",
       " 'byproduct': 3,\n",
       " 'dennis': 66,\n",
       " \"'uexact\": 2,\n",
       " 'stoking': 1,\n",
       " 'evened': 4,\n",
       " 'corker': 1,\n",
       " '1868': 2,\n",
       " 'avidly': 3,\n",
       " 'namesake': 2,\n",
       " 'rape!': 4,\n",
       " 'reactionary': 2,\n",
       " 'quells': 1,\n",
       " 'excused': 12,\n",
       " 'suffused': 2,\n",
       " 'perched': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
    "threshold_questions = 20\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word_dict.items():\n",
    "    if count >= threshold_questions:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "threshold_answers = 20\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word_dict.items():\n",
    "    if count >= threshold_answers:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the last tokens to these two dictionaries\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inverse dictionary of the answerswords2int dictionary\n",
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the End Of String token to the end of every answer\n",
    "for i in range(len(answers_clean)):\n",
    "    answers_clean[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating all the questions and the answers into integers\n",
    "# and Replacing all the words that were filtered out by <OUT> \n",
    "questions_into_int = []\n",
    "for question in questions_clean:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in answers_clean:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting questions and answers by the length of questions\n",
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answersints2word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the seq-seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the function to make placeholder for input and the output.\n",
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None , None], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32, [None , None], name=\"targets\")    \n",
    "    lr = tf.placeholder(tf.int32, name=\"learning_rate\")\n",
    "    dropout = tf.placeholder(tf.float32, name=\"dropout\")\n",
    "    return inputs, targets, lr, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the targets\n",
    "def preprocess_targets(targets, word_dict, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word_dict['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Encoder RNN\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the test/validation set\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the seq2seq model\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5\n",
    "\n",
    "\n",
    "# Defining a session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    "\n",
    "# Setting the sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    "\n",
    "# Getting the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
    "\n",
    "# Padding the sequences with the <PAD> token\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
    "\n",
    "# Splitting the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch\n",
    "\n",
    "# Splitting the questions and answers into training and validation sets\n",
    "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answers = sorted_clean_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"chatbot_weights.ckpt\" # For Windows users, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
